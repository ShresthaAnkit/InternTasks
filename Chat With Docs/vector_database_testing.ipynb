{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import wikipedia\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record the start time\n",
    "        result = func(*args, **kwargs)  # Call the wrapped function\n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "        print(f\"Function '{func.__name__}' took {elapsed_time:.6f} seconds to execute.\")\n",
    "        return result  # Return the result of the wrapped function\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\",\".\",\"\\n\", \" \"],  # Prioritize splitting by paragraphs, then sentences, then spaces\n",
    "    chunk_size=300,  # Maximum size of each chunk\n",
    "    chunk_overlap=50  # Overlap to maintain context between chunks\n",
    ")\n",
    "\n",
    "# Clean the individual chunks\n",
    "def clean_chunk(chunk):\n",
    "    cleaned_chunk = chunk.strip()  # Remove leading/trailing whitespace\n",
    "    cleaned_chunk = re.sub(r\"\\n+\", \" \", cleaned_chunk)\n",
    "    cleaned_chunk = re.sub(r\"\\s+\", \" \", cleaned_chunk)\n",
    "    cleaned_chunk = re.sub(r'(?:==|===|=).*?(?==|===|=|$)', '', cleaned_chunk)\n",
    "    if cleaned_chunk.startswith(\".\"):\n",
    "        cleaned_chunk = cleaned_chunk[1:]  # Remove the first character (the period)\n",
    "    return cleaned_chunk.strip()\n",
    "\n",
    "# # Apply the cleaning function to each chunk\n",
    "# chunks = text_splitter.split_text(page.content)\n",
    "# cleaned_chunks = [clean_chunk(chunk) for chunk in chunks] \n",
    "# cleaned_chunks = [chunk for chunk in cleaned_chunks if len(chunk) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_it\n",
    "def scrape_wiki():\n",
    "    temp_results = wikipedia.search(\"Programming\")\n",
    "    temp_results2 = wikipedia.search(\"Universe\")\n",
    "    results = []\n",
    "    for temp in temp_results:\n",
    "        results.extend(wikipedia.search(temp))\n",
    "    for temp in temp_results2:\n",
    "        results.extend(wikipedia.search(temp))\n",
    "    results = list(set(results))\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for result in results:\n",
    "        try:\n",
    "            temp_result = wikipedia.page(result)\n",
    "        except wikipedia.DisambiguationError as e:\n",
    "            # Select the first option from the disambiguation list\n",
    "            try:\n",
    "                temp_result = wikipedia.page(e.options[0])\n",
    "            except wikipedia.PageError:\n",
    "                print(f\"Page not found for {e.options[0]}\")\n",
    "                continue  # Skip to the next result\n",
    "        except wikipedia.PageError:\n",
    "            print(f\"Page not found for {result}\")\n",
    "            continue  # Skip to the next result\n",
    "\n",
    "        chunks = text_splitter.split_text(temp_result.content)\n",
    "        cleaned_chunk = [clean_chunk(chunk) for chunk in chunks]\n",
    "        cleaned_chunks.extend(cleaned_chunk)\n",
    "\n",
    "    cleaned_chunks = [chunk for chunk in cleaned_chunks if len(chunk) > 1]\n",
    "    return cleaned_chunks\n",
    "#cleaned_chunks = scrape_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_chunks.txt', 'w',encoding='utf-8') as file:\n",
    "    for item in cleaned_chunks:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chunks = []\n",
    "with open('cleaned_chunks.txt', 'r', encoding='utf-8') as file:\n",
    "    cleaned_chunks = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = []\n",
    "for i in range(len(cleaned_chunks)):\n",
    "    wiki_data.append(\n",
    "        {\n",
    "            'text':cleaned_chunks[i],\n",
    "            'vector':embeddings[i].tolist()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHROMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name='Programming_Universe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With embedder in chroma itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:35<00:00, 22.35s/it]\n"
     ]
    }
   ],
   "source": [
    "@time_it\n",
    "def chroma_insert_with_embedder(cleaned_chunks):\n",
    "    collection = chroma_client.create_collection(name=\"Programming_Universe\",embedding_function=sentence_transformer_ef)\n",
    "    for i in tqdm(range(0, len(cleaned_chunks), 1000)):\n",
    "        batch = [s for s in cleaned_chunks[i:i + 1000]]\n",
    "        id_batch = [f'chunk_{i}_{j}' for j in range(len(cleaned_chunks[i:i+1000]))]\n",
    "        collection.add(\n",
    "            documents=batch,\n",
    "            ids=id_batch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without embedder in Chroma, text are manually embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'chroma_insert_without_embedder' took 10.190860 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Collection(name=Programming_Universe)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time_it\n",
    "def chroma_insert_without_embedder(wiki_data):\n",
    "    collection = chroma_client.create_collection(name=\"Programming_Universe\")\n",
    "    for i in range(0, len(wiki_data), 1000):\n",
    "        batch = [s['text'] for s in wiki_data[i:i + 1000]]\n",
    "        id_batch = [f'chunk_{i}_{j}' for j in range(len(wiki_data[i:i+1000]))]\n",
    "        embedding_batch = [s['vector'] for s in wiki_data[i:i+1000]]\n",
    "        collection.add(\n",
    "            documents=batch,\n",
    "            ids=id_batch,\n",
    "            embeddings=embedding_batch\n",
    "        )\n",
    "    return collection\n",
    "#chroma_client.delete_collection(name='Programming_Universe')\n",
    "chroma_insert_without_embedder(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is pydoc?', 'What is PyS60?','What does it claim to strive?','What are the main goals?','What are the core philosophies?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'chroma_query' took 0.031608 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_1000_524',\n",
       "   'chunk_1000_572',\n",
       "   'chunk_4000_526',\n",
       "   'chunk_1000_704',\n",
       "   'chunk_4000_604']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected',\n",
       "   'Python is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation',\n",
       "   'The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted (by both the runtime system and by human readers). The Python language has many similarities to Perl, C, and Java',\n",
       "   'Python is used extensively in the information security industry, including in exploit development. Most of the Sugar software for the One Laptop per Child XO, developed at Sugar Labs as of 2008, is written in Python',\n",
       "   \"And everything in Python is an object, including classes, functions, numbers and modules. Python also has support for metaclasses, an advanced tool for enhancing classes' functionality. Naturally, inheritance, including multiple inheritance, is supported\"]],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None, None, None]],\n",
       " 'distances': [[0.22877633571624756,\n",
       "   0.36776646971702576,\n",
       "   0.3982989490032196,\n",
       "   0.44187647104263306,\n",
       "   0.4440959095954895]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time_it\n",
    "def chroma_query(query):\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    collection = chroma_client.get_collection('Programming_Universe')\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding], \n",
    "        n_results=5 # how many results to return\n",
    "    )\n",
    "    return results\n",
    "chroma_query('What is python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANCEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the LanceDB database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "\n",
    "# Define the schema for the table\n",
    "class Words(LanceModel):\n",
    "    text: str \n",
    "    vector: Vector(384)\n",
    "\n",
    "# Create the table if it doesn't already exist\n",
    "table = db.create_table(\"Programming_Universe_lancedb\", schema=Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the table named \"words\"\n",
    "db.drop_table(\"Programming_Universe_lancedb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'lancedb_insert' took 0.429397 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "@time_it\n",
    "def lancedb_insert(strings, batch_size=1000):\n",
    "    for i in range(0, len(strings), batch_size):\n",
    "        batch = [s for s in strings[i:i + batch_size]]\n",
    "        table.add(batch)\n",
    "        #print(f\"Added batch {i // batch_size + 1} with {len(batch)} records.\")\n",
    "lancedb_insert(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'lancedb_query' took 0.037681 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Words(text='Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected', vector=FixedSizeList(dim=384))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time_it\n",
    "def lancedb_query(query_string):      \n",
    "    query_embedding = model.encode(query_string).tolist()  \n",
    "    results = table.search(query_embedding)    \n",
    "    return results.limit(1).to_pydantic(Words)\n",
    "lancedb_query(\"What is python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAVIATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_objs = list()\n",
    "for i, d in enumerate(wiki_data):\n",
    "    wiki_objs.append(wvc.data.DataObject(\n",
    "        properties={\n",
    "            \"text\": d[\"text\"],            \n",
    "        },\n",
    "        vector=d[\"vector\"]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.collections.delete(name=\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes as wvc\n",
    "\n",
    "# Create the collection. Weaviate's autoschema feature will infer properties when importing.\n",
    "weaviate_collection = weaviate_client.collections.create(\n",
    "    \"Python\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'weaviate_insert' took 4.309588 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "@time_it\n",
    "def weaviate_insert(wiki_objs):\n",
    "    weaviate_collection = weaviate_client.collections.get(\"Python\")\n",
    "    for i in range(0, len(wiki_objs), 1000):\n",
    "        batch = wiki_objs[i:i + 1000]\n",
    "        weaviate_collection.data.insert_many(batch)\n",
    "weaviate_insert(wiki_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'weaviate_query' took 0.044614 seconds to execute.\n",
      "{'text': 'Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected'}\n"
     ]
    }
   ],
   "source": [
    "@time_it\n",
    "def weaviate_query(query):\n",
    "    query_vector = model.encode(query)\n",
    "\n",
    "    response = weaviate_collection.query.near_vector(\n",
    "        near_vector=query_vector.tolist(),\n",
    "        limit=2,\n",
    "        return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    "    )\n",
    "    return response\n",
    "response = weaviate_query('What is python')\n",
    "\n",
    "print(response.objects[0].properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For using llama model and nomic embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "questions = client.collections.create(\n",
    "    name=\"WikipediaPage\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama(     # Configure the Ollama embedding integration\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "        model=\"nomic-embed-text\",                               # The model to use\n",
    "    ),\n",
    "    generative_config=Configure.Generative.ollama(              # Configure the Ollama generative integration\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "        model=\"llama3.2\",                                       # The model to use\n",
    "    )\n",
    ")\n",
    "\n",
    "client.close()  # Free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
