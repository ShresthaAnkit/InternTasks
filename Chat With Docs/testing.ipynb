{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import numpy as np\n",
    "import cohere\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages(input_pdf, output_pdf, pages_to_extract):\n",
    "    # Create a PDF reader and writer\n",
    "    reader = PdfReader(input_pdf)\n",
    "    writer = PdfWriter()\n",
    "    \n",
    "    # Extract the specified pages\n",
    "    for page_num in pages_to_extract:\n",
    "        writer.add_page(reader.pages[page_num])\n",
    "\n",
    "    # Write the extracted pages to a new PDF\n",
    "    with open(output_pdf, 'wb') as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "    print(f\"New PDF created: {output_pdf}\")\n",
    "\n",
    "\n",
    "# input_pdf = r'D:\\Programming\\AI\\Basics\\AMNIL Tech\\Chat With Docs\\energy.pdf' \n",
    "# output_pdf = 'sample.pdf'       \n",
    "# pages_to_extract = [0,1]           \n",
    "\n",
    "# extract_pages(input_pdf, output_pdf, pages_to_extract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'BERT.pdf'\n",
    "reader = PdfReader(pdf_path)\n",
    "page = reader.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'BERT: Pre-training of Deep Bidirectional Transformers for\\n', Font Size: 14.3462\n",
      "Text: 'Language Understanding', Font Size: 14.3462\n",
      "Text: '\\n', Font Size: 11.9552\n",
      "Text: 'Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova', Font Size: 11.9552\n",
      "Text: '\\n', Font Size: 11.9552\n",
      "Text: 'Google AI Language', Font Size: 11.9552\n",
      "Text: '\\n', Font Size: 11.9552\n",
      "Text: 'f', Font Size: 11.9552\n",
      "Text: 'jacobdevlin,mingweichang,kentonl,kristout', Font Size: 11.9552\n",
      "Text: ' g', Font Size: 11.9552\n",
      "Text: '@google.com', Font Size: 11.9552\n",
      "Text: '\\n', Font Size: 11.9552\n",
      "Text: 'Abstract', Font Size: 11.9552\n",
      "Text: '\\n', Font Size: 9.9626\n",
      "Text: 'We introduce a new language representa-\\n', Font Size: 9.9626\n",
      "Text: 'tion model called', Font Size: 9.9626\n",
      "Text: ' BERT', Font Size: 9.9626\n",
      "Text: ' , which stands for', Font Size: 9.9626\n",
      "Text: '\\n', Font Size: 9.9626\n",
      "Text: 'B', Font Size: 9.9626\n",
      "Text: 'idirectional', Font Size: 9.9626\n",
      "Text: ' E', Font Size: 9.9626\n",
      "Text: 'ncoder', Font Size: 9.9626\n",
      "Text: ' R', Font Size: 9.9626\n",
      "Text: 'epresentations from', Font Size: 9.9626\n",
      "Text: '\\n', Font Size: 9.9626\n",
      "Text: 'T', Font Size: 9.9626\n",
      "Text: 'ransformers. Unlike recent language repre-\\n', Font Size: 9.9626\n",
      "Text: 'sentation models (Peters et al., 2018a; Rad-\\n', Font Size: 9.9626\n",
      "Text: 'ford et al., 2018), BERT is designed to pre-\\n', Font Size: 9.9626\n",
      "Text: 'train deep bidirectional representations from\\n', Font Size: 9.9626\n",
      "Text: 'unlabeled text by jointly conditioning on both\\n', Font Size: 9.9626\n",
      "Text: 'left and right context in all layers. As a re-\\n', Font Size: 9.9626\n",
      "Text: 'sult, the pre-trained BERT model can be ﬁne-\\n', Font Size: 9.9626\n",
      "Text: 'tuned with just one additional output layer\\n', Font Size: 9.9626\n",
      "Text: 'to create state-of-the-art models for a wide\\n', Font Size: 9.9626\n",
      "Text: 'range of tasks, such as question answering and\\n', Font Size: 9.9626\n",
      "Text: 'language inference, without substantial task-\\n', Font Size: 9.9626\n",
      "Text: 'speciﬁc architecture modiﬁcations.\\n', Font Size: 9.9626\n",
      "Text: 'BERT is conceptually simple and empirically\\n', Font Size: 9.9626\n",
      "Text: 'powerful. It obtains new state-of-the-art re-\\n', Font Size: 9.9626\n",
      "Text: 'sults on eleven natural language processing\\n', Font Size: 9.9626\n",
      "Text: 'tasks, including pushing the GLUE score to\\n', Font Size: 9.9626\n",
      "Text: '80.5% (7.7% point absolute improvement),\\n', Font Size: 9.9626\n",
      "Text: 'MultiNLI accuracy to 86.7% (4.6% absolute\\n', Font Size: 9.9626\n",
      "Text: 'improvement), SQuAD v1.1 question answer-\\n', Font Size: 9.9626\n",
      "Text: 'ing Test F1 to 93.2 (1.5 point absolute im-\\n', Font Size: 9.9626\n",
      "Text: 'provement) and SQuAD v2.0 Test F1 to 83.1\\n', Font Size: 9.9626\n",
      "Text: '(5.1 point absolute improvement).', Font Size: 9.9626\n",
      "Text: '\\n', Font Size: 11.9552\n",
      "Text: '1 Introduction', Font Size: 11.9552\n",
      "Text: '\\n', Font Size: 10.9091\n",
      "Text: 'Language model pre-training has been shown to\\n', Font Size: 10.9091\n",
      "Text: 'be effective for improving many natural language\\n', Font Size: 10.9091\n",
      "Text: 'processing tasks (Dai and Le, 2015; Peters et al.,\\n', Font Size: 10.9091\n",
      "Text: '2018a; Radford et al., 2018; Howard and Ruder,\\n', Font Size: 10.9091\n",
      "Text: '2018). These include sentence-level tasks such as\\n', Font Size: 10.9091\n",
      "Text: 'natural language inference (Bowman et al., 2015;\\n', Font Size: 10.9091\n",
      "Text: 'Williams et al., 2018) and paraphrasing (Dolan\\n', Font Size: 10.9091\n",
      "Text: 'and Brockett, 2005), which aim to predict the re-\\n', Font Size: 10.9091\n",
      "Text: 'lationships between sentences by analyzing them\\n', Font Size: 10.9091\n",
      "Text: 'holistically, as well as token-level tasks such as\\n', Font Size: 10.9091\n",
      "Text: 'named entity recognition and question answering,\\n', Font Size: 10.9091\n",
      "Text: 'where models are required to produce ﬁne-grained\\n', Font Size: 10.9091\n",
      "Text: 'output at the token level (Tjong Kim Sang and\\n', Font Size: 10.9091\n",
      "Text: 'De Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\n', Font Size: 10.9091\n",
      "Text: 'ing pre-trained language representations to down-\\n', Font Size: 10.9091\n",
      "Text: 'stream tasks:', Font Size: 10.9091\n",
      "Text: ' feature-based', Font Size: 10.9091\n",
      "Text: ' and', Font Size: 10.9091\n",
      "Text: 'ﬁne-tuning', Font Size: 10.9091\n",
      "Text: ' . The\\n', Font Size: 10.9091\n",
      "Text: 'feature-based approach, such as ELMo (Peters\\n', Font Size: 10.9091\n",
      "Text: 'et al., 2018a), uses task-speciﬁc architectures that\\n', Font Size: 10.9091\n",
      "Text: 'include the pre-trained representations as addi-\\n', Font Size: 10.9091\n",
      "Text: 'tional features. The ﬁne-tuning approach, such as\\n', Font Size: 10.9091\n",
      "Text: 'the Generative Pre-trained Transformer (OpenAI\\n', Font Size: 10.9091\n",
      "Text: 'GPT) (Radford et al., 2018), introduces minimal\\n', Font Size: 10.9091\n",
      "Text: 'task-speciﬁc parameters, and is trained on the\\n', Font Size: 10.9091\n",
      "Text: 'downstream tasks by simply ﬁne-tuning', Font Size: 10.9091\n",
      "Text: ' all', Font Size: 10.9091\n",
      "Text: 'pre-\\n', Font Size: 10.9091\n",
      "Text: 'trained parameters. The two approaches share the\\n', Font Size: 10.9091\n",
      "Text: 'same objective function during pre-training, where\\n', Font Size: 10.9091\n",
      "Text: 'they use unidirectional language models to learn\\n', Font Size: 10.9091\n",
      "Text: 'general language representations.\\n', Font Size: 10.9091\n",
      "Text: 'We argue that current techniques restrict the\\n', Font Size: 10.9091\n",
      "Text: 'power of the pre-trained representations, espe-\\n', Font Size: 10.9091\n",
      "Text: 'cially for the ﬁne-tuning approaches. The ma-\\n', Font Size: 10.9091\n",
      "Text: 'jor limitation is that standard language models are\\n', Font Size: 10.9091\n",
      "Text: 'unidirectional, and this limits the choice of archi-\\n', Font Size: 10.9091\n",
      "Text: 'tectures that can be used during pre-training. For\\n', Font Size: 10.9091\n",
      "Text: 'example, in OpenAI GPT, the authors use a left-to-\\n', Font Size: 10.9091\n",
      "Text: 'right architecture, where every token can only at-\\n', Font Size: 10.9091\n",
      "Text: 'tend to previous tokens in the self-attention layers\\n', Font Size: 10.9091\n",
      "Text: 'of the Transformer (Vaswani et al., 2017). Such re-\\n', Font Size: 10.9091\n",
      "Text: 'strictions are sub-optimal for sentence-level tasks,\\n', Font Size: 10.9091\n",
      "Text: 'and could be very harmful when applying ﬁne-\\n', Font Size: 10.9091\n",
      "Text: 'tuning based approaches to token-level tasks such\\n', Font Size: 10.9091\n",
      "Text: 'as question answering, where it is crucial to incor-\\n', Font Size: 10.9091\n",
      "Text: 'porate context from both directions.\\n', Font Size: 10.9091\n",
      "Text: 'In this paper, we improve the ﬁne-tuning based\\n', Font Size: 10.9091\n",
      "Text: 'approaches by proposing BERT:', Font Size: 10.9091\n",
      "Text: ' B', Font Size: 10.9091\n",
      "Text: 'idirectional', Font Size: 10.9091\n",
      "Text: '\\n', Font Size: 10.9091\n",
      "Text: 'E', Font Size: 10.9091\n",
      "Text: 'ncoder', Font Size: 10.9091\n",
      "Text: ' R', Font Size: 10.9091\n",
      "Text: 'epresentations from', Font Size: 10.9091\n",
      "Text: ' T', Font Size: 10.9091\n",
      "Text: 'ransformers.\\n', Font Size: 10.9091\n",
      "Text: 'BERT alleviates the previously mentioned unidi-\\n', Font Size: 10.9091\n",
      "Text: 'rectionality constraint by using a “masked lan-\\n', Font Size: 10.9091\n",
      "Text: 'guage model” (MLM) pre-training objective, in-\\n', Font Size: 10.9091\n",
      "Text: 'spired by the Cloze task (Taylor, 1953). The\\n', Font Size: 10.9091\n",
      "Text: 'masked language model randomly masks some of\\n', Font Size: 10.9091\n",
      "Text: 'the tokens from the input, and the objective is to\\n', Font Size: 10.9091\n",
      "Text: 'predict the original vocabulary id of the masked', Font Size: 10.9091\n",
      "Text: 'arXiv:1810.04805v2  [cs.CL]  24 May 2019', Font Size: 20.0\n"
     ]
    }
   ],
   "source": [
    "parts = []\n",
    "def visitor_body(text, cm, tm, fontDict, fontSize):\n",
    "    y = tm[5]        \n",
    "    if y > 50 and y < 800:\n",
    "        print(f\"Text: {repr(text)}, Font Size: {fontSize}\")\n",
    "        parts.append(text)\n",
    "page.extract_text(visitor_text=visitor_body)\n",
    "text_body = \"\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(text, max_length):\n",
    "    # Regular expression to match sentence-ending punctuation: . ? !\n",
    "    sentence_endings = re.compile(r'([.!?])')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Split the text into sentences based on the sentence-ending punctuation\n",
    "    sentences = sentence_endings.split(text)\n",
    "    \n",
    "    # Recombine punctuation with the sentences\n",
    "    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '') for i in range(0, len(sentences), 2)]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If adding the sentence exceeds max_length, start a new chunk\n",
    "        if len(current_chunk) + len(sentence) > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = sentence.strip()\n",
    "        else:\n",
    "            current_chunk += sentence  # Add the sentence to the current chunk\n",
    "\n",
    "    if current_chunk:  # Append the last chunk\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language fjacobdevlin,mingweichang,kentonl,kristout g@google.com Abstract We introduce a new language representa- tion model called BERT , which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al.\n",
      "\n",
      "Chunk 2:\n",
      ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be ﬁne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speciﬁc architecture modiﬁcations. BERT is conceptually simple and empirically powerful.\n",
      "\n",
      "Chunk 3:\n",
      "It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\n",
      "Chunk 4:\n",
      "1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al.\n",
      "\n",
      "Chunk 5:\n",
      ", 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce ﬁne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n",
      "\n",
      "Chunk 6:\n",
      "There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based andﬁne-tuning . The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciﬁc architectures that include the pre-trained representations as addi- tional features. The ﬁne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al.\n",
      "\n",
      "Chunk 7:\n",
      ", 2018), introduces minimal task-speciﬁc parameters, and is trained on the downstream tasks by simply ﬁne-tuning allpre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the ﬁne-tuning approaches.\n",
      "\n",
      "Chunk 8:\n",
      "The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).\n",
      "\n",
      "Chunk 9:\n",
      "Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying ﬁne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the ﬁne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.\n",
      "\n",
      "Chunk 10:\n",
      "BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the maskedarXiv:1810.04805v2 [cs.CL] 24 May 2019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_by_sentence(text_body,500)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language fjacobdevlin,mingweichang,kentonl,kristout g@google.com Abstract We introduce a new language representa- tion model called BERT , which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al.',\n",
       " ', 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be ﬁne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speciﬁc architecture modiﬁcations. BERT is conceptually simple and empirically powerful.',\n",
       " 'It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       " '1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al.',\n",
       " ', 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce ﬁne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).',\n",
       " 'There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based andﬁne-tuning . The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciﬁc architectures that include the pre-trained representations as addi- tional features. The ﬁne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al.',\n",
       " ', 2018), introduces minimal task-speciﬁc parameters, and is trained on the downstream tasks by simply ﬁne-tuning allpre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the ﬁne-tuning approaches.',\n",
       " 'The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).',\n",
       " 'Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying ﬁne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the ﬁne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.',\n",
       " 'BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the maskedarXiv:1810.04805v2 [cs.CL] 24 May 2019']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_KEY = os.getenv('COHERE_KEY')\n",
    "co = cohere.Client(COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"embed-english-v3.0\"\n",
    "input_type = \"search_document\"\n",
    "\n",
    "chunk_embeddings = co.embed(\n",
    "    texts = chunks,\n",
    "    model = model,\n",
    "    input_type = input_type,\n",
    "    embedding_types=['float']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type = 'search_query'\n",
    "query = 'What are the major limitations of standard lanugage models?'\n",
    "query_embedding = co.embed(\n",
    "    texts = [query],\n",
    "    model = model,\n",
    "    input_type = input_type,\n",
    "    embedding_types= ['float']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14988067304180056\n",
      "0.1956473193532332\n",
      "0.17145194664191885\n",
      "0.17134736578339163\n",
      "0.19394368632370673\n",
      "0.19504123330431916\n",
      "0.28432602192053075\n",
      "0.4630081800950562\n",
      "0.29434924040318294\n",
      "0.32752604537078445\n"
     ]
    }
   ],
   "source": [
    "chunk_embedding_floats = chunk_embeddings.embeddings.float\n",
    "query_embedding_float = query_embedding.embeddings.float[0]\n",
    "for chunk_embedding_float in chunk_embedding_floats:\n",
    "    print(calculate_similarity(chunk_embedding_float,query_embedding_float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prompt for the model\n",
    "prompt = \"\"\"\n",
    "Use only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.\n",
    "Context: {chunks[7]} and {chunks[9]}\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Generate text\n",
    "response = co.generate(\n",
    "    model=\"command\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=200,  \n",
    "    temperature=0.7,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the maskedarXiv:1810.04805v2 [cs.CL] 24 May 2019'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The major limitation with standard language models is that they are typically unidirectional, which limits the kinds of architectures that can be used during pretraining.  OpenAI GPT uses a left-to-right architecture, where each token can only attend to previous tokens in the self-attention layers of the Transformer.  BERT is an example of an architecture that works around this limitation by using a masked language model pretraining objective inspired by the Cloze task, where the model randomly masks input tokens and predicts the masked tokens, increasing bidirectionality.  This allows BERT to outperform previous models on a number of tasks.  Overall, bidirectionality is a key aspect of more advanced language models that greatly improves their performance.  Without this limitation, the potential of the models increases. \n"
     ]
    }
   ],
   "source": [
    "# Access the generated text\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases = [\"i love soup\", \"soup is my favorite\", \"london is far away\"]\n",
    "# model = \"embed-english-v3.0\"\n",
    "# input_type = \"search_query\"\n",
    "# res = co.embed(\n",
    "#     texts=phrases,\n",
    "#     model=model,\n",
    "#     input_type=input_type,\n",
    "#     embedding_types=[\"float\"],\n",
    "# )\n",
    "# (soup1, soup2, london) = res.embeddings.float\n",
    "# calculate_similarity(soup1, soup2)  # 0.85 \n",
    "# calculate_similarity(soup1, london) # 0.164789"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
