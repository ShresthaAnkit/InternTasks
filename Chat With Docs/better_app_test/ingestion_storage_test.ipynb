{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings using OpenAI\n",
    "# text-embedding-ada-002\n",
    "def generate_embeddings(texts):        \n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embeddings(query):\n",
    "    response = client.embeddings.create(input=query, model=\"text-embedding-ada-002\")\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the LanceDB database\n",
    "db = lancedb.connect(\"mydb\")\n",
    "\n",
    "class KnowledgeBase(LanceModel):\n",
    "    kb_id: str  # Unique knowledge base ID\n",
    "    name: str  # Knowledge base name\n",
    "    description: str  # Optional description\n",
    "    model: str  # Embedding model used\n",
    "\n",
    "class Chunk(LanceModel):\n",
    "    chunk_id: str  # Unique chunk ID\n",
    "    kb_id: str  # Foreign key to `KnowledgeBase`    \n",
    "    text: str  # Chunked text\n",
    "    vector: Vector(1536)  # Embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.drop_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table if it doesn't already exist\n",
    "if \"KnowledgeBase\" not in db.table_names():\n",
    "    table = db.create_table(\"KnowledgeBase\", schema=KnowledgeBase)\n",
    "if \"Chunk\" not in db.table_names():\n",
    "    table = db.create_table(\"Chunk\", schema=Chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition(r\"D:\\Programming\\Python\\AI\\Basics\\AMNIL Tech\\Chat With Docs\\better_app_test\\energy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\\n\\n\".join([str(el) for el in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create an instance of RecursiveCharacterTextSplitter\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Define the chunk size\n",
    "    chunk_overlap=100,  # Define the overlap size\n",
    "    length_function=len  # Defines how length is measured\n",
    ")\n",
    "with open('data.txt','r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "#Split the document into chunks\n",
    "chunks = recursive_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks = generate_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1536)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(embedded_chunks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = db.open_table(\"KnowledgeBase\")\n",
    "table.create_fts_index(\"name\", use_tantivy=False,replace=True)\n",
    "table.search(\"DWBI\",vector_column_name='name').select([\"kb_id\"]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_chunks(chunks, embedded_chunks,KB_NAME,model):\n",
    "    kb_table = db.open_table('KnowledgeBase')\n",
    "    kb_table.create_fts_index(\"name\", use_tantivy=False,replace=True)\n",
    "    existing_kb = kb_table.search(KB_NAME,vector_column_name='name').select([\"kb_id\"]).to_list()\n",
    "    if existing_kb:\n",
    "        print(f\"Knowledge base '{KB_NAME}' already exists.\")\n",
    "    else:    \n",
    "        kb_id=str(uuid.uuid4())\n",
    "        # Create a new knowledge base\n",
    "        kb_table.add([KnowledgeBase(\n",
    "            kb_id=kb_id,\n",
    "            name=KB_NAME,\n",
    "            description=\"A knowledge base for DWBI\",\n",
    "            model=model\n",
    "        )])\n",
    "        chunk_table = db.open_table('Chunk')\n",
    "        # Save each chunk and its vector into LanceDB\n",
    "        for chunk, embedding in zip(chunks, embedded_chunks):\n",
    "            padded_vector = np.pad(embedding, (0, 1536 - len(embedding)), 'constant', constant_values=0)\n",
    "            chunk_table.add([Chunk(chunk_id=str(uuid.uuid4()),kb_id=kb_id,text=chunk, vector=padded_vector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_chunks(chunks, embedded_chunks,KB_NAME='DWBI',model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kb_id(KB_NAME):\n",
    "    try:\n",
    "        table = db.open_table(\"KnowledgeBase\")\n",
    "        table.create_fts_index(\"name\", use_tantivy=False,replace=True)\n",
    "        return table.search(KB_NAME,vector_column_name='name').select([\"kb_id\"]).to_list()[0]['kb_id']    \n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_KB(KB_NAME):    \n",
    "    kb_id = get_kb_id(KB_NAME)\n",
    "    if kb_id is None:\n",
    "        return None\n",
    "    kb_table = db.open_table(\"Chunk\")\n",
    "    kb_table.create_fts_index(\"kb_id\", use_tantivy=False,replace=True)\n",
    "    chunk_df = kb_table.search(kb_id,vector_column_name='kb_id').select([\"chunk_id\",\"kb_id\",\"text\",\"vector\"]).to_pandas()    \n",
    "    return chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chunks(query,chunk_df):\n",
    "    embedded_query = generate_query_embeddings(query) \n",
    "    chunk_df['similarity'] = chunk_df.apply(lambda x: calculate_similarity(embedded_query,x.iloc[3]),axis=1)    \n",
    "    df_top = chunk_df[chunk_df['similarity'] > chunk_df['similarity'].quantile(0.8)]      \n",
    "    context = ''\n",
    "    for txt in df_top['text']:\n",
    "        context+=txt\n",
    "        context+='\\n'\n",
    "    prompt = f\"\"\"\n",
    "    \"QUESTION:\" {query}\\n\n",
    "    \"CONTEXT:\" {context}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    premable = 'You are a friendly bot. Don\\'t ask for extra context. Read the \"QUESTION:\" and reply. If the question demands some information, answer the question provided as \"QUESTION:\" using the context provided as \"CONTEXT:\" If the answer is not present, say you don\\'t know.'\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=[{'role':'system','content':premable},{'role':'user','content':prompt}],\n",
    "        max_tokens=150,  # You can adjust the max tokens based on your needs\n",
    "        temperature=0.5,  # Adjust temperature for randomness (optional)\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(KB_NAME,query):\n",
    "    chunk_df = retrieve_KB(KB_NAME)\n",
    "    if chunk_df is None:\n",
    "        return 'No Knowledge base found.'\n",
    "    chunk_df.drop(columns=['_score'],inplace=True)\n",
    "    prompt = query_chunks(query,chunk_df)    \n",
    "    response = generate_response(prompt)    \n",
    "    generated_text = response.choices[0].message.content\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BI applications get their data from large amounts of structured and sometimes unstructured data.'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond('DWBI',\"Where do BI applications get their data?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amniltech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
