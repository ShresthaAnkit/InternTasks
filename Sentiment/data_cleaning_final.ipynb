{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['length'] = df_train['text'].apply(lambda x: len(x))\n",
    "# df_test['length'] = df_test['text'].apply(lambda x: len(x))\n",
    "# df_train = df_train[df_train['length']<=256]\n",
    "# df_test = df_test[df_test['length']<=256]\n",
    "# df_train.drop(columns=['length'], inplace=True,axis=1)\n",
    "# df_test.drop(columns=['length'], inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to remove punctuations and other special characters including repeating white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    text = re.sub(r'[+!@#$%^&*(),.?\":{}|<>~`;/\\\\[\\]\\'|-]+', ' ', text) # Remove all special characters    \n",
    "    text = re.sub(r'।', ' ', text)  # Remove the '।' character\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra whitespace    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(nltk.corpus.stopwords.words('nepali'))\n",
    "import codecs\n",
    "\n",
    "# Initialize an empty list for extra stopwords\n",
    "extra_stop_words = []\n",
    "\n",
    "# Open the file with UTF-8 encoding\n",
    "with codecs.open('nepali_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip whitespace and add the word to the list\n",
    "        extra_stop_words.append(line.strip())\n",
    "stopwords_list.update(extra_stop_words)\n",
    "stopwords_list = set(stopwords_list)\n",
    "stopwords_list.remove('राम्रो')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = text.lower()\n",
    "    filtered_words = [word for word in text.split() if word not in stopwords_list]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "# Function to replace emojis with descriptions and add spaces\n",
    "def replace_emojis_with_space(text):\n",
    "    # Replace emojis with descriptions, adding spaces before and after each description\n",
    "    text = demoji.replace_with_desc(text, sep=\"+\")  # Add space between descriptions\n",
    "    text = text.replace('-',' ')\n",
    "    text = text.replace(':',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_between_plus(text):\n",
    "    # Use regex to find all words between + and +\n",
    "    return re.findall(r'\\+(.*?)\\+', text)\n",
    "def contains_english_letters(text):\n",
    "    # Check if the text contains at least one English letter (a-z or A-Z)\n",
    "    return bool(re.search(r'[a-zA-Z]', text))\n",
    "def find_english_words(text):\n",
    "    # Check if the text contains at least one English letter (a-z or A-Z)\n",
    "    return re.findall(r'[a-zA-Z\\+]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_english_to_nepali(batch):\n",
    "    return GoogleTranslator(source='en', target='ne').translate_batch(batch)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert emojis to english, extract them, map them to nepali words and replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_extractor(df):\n",
    "    present_english_words = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        sentence = ' '.join(find_english_words(text))\n",
    "        unique_words = set(find_words_between_plus(sentence))\n",
    "        if unique_words:            \n",
    "            present_english_words.extend(unique_words)\n",
    "    return set(present_english_words)\n",
    "def emoji_handler(df_train,df_test):\n",
    "    emoji_words = emoji_extractor(df_train)\n",
    "    emoji_words_test = emoji_extractor(df_test)\n",
    "    total_words = emoji_words.union(emoji_words_test)\n",
    "    total_words_list = list(total_words)\n",
    "    translated_emojis = translate_english_to_nepali(total_words_list)\n",
    "    emoji_dict = dict(zip(total_words_list, translated_emojis))\n",
    "    df_train['text'] = df_train['text'].replace({'\\+': ' '}, regex=True)\n",
    "    df_train['text'] = df_train['text'].replace(emoji_dict,regex=True)\n",
    "    df_test['text'] = df_test['text'].replace({'\\+': ' '}, regex=True)\n",
    "    df_test['text'] = df_test['text'].replace(emoji_dict,regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to process English words after emojis have been dealth with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract English words and separate sentences based on Nepali characters\n",
    "def extract_english_words_inbetween(text):\n",
    "    # Regex pattern for detecting Nepali characters and English words\n",
    "    nepali_pattern = r'[\\u0900-\\u097F]+'\n",
    "    english_word_pattern = r'[a-zA-Z]+'\n",
    "    \n",
    "    # Split text by Nepali characters to separate sentences\n",
    "    sentences = re.split(nepali_pattern, text)\n",
    "    \n",
    "    # Now extract English words from each sentence\n",
    "    english_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = re.findall(english_word_pattern, sentence)\n",
    "        if words:  # If there are any English words, store them as a sentence\n",
    "            english_sentences.append(\" \".join(words))\n",
    "    \n",
    "    return english_sentences\n",
    "# Extracts all english words. If they are separated by Nepali characters they are taken as a different sentence\n",
    "def misc_english_extractor(df):\n",
    "    present_english_words = []\n",
    "    for text in df['text']:\n",
    "        eng = extract_english_words_inbetween(text)\n",
    "        if eng:\n",
    "            present_english_words.extend(eng)\n",
    "    return set(present_english_words)\n",
    "def misc_english_handler(df_train,df_test):\n",
    "    misc_english_words = misc_english_extractor(df_train)\n",
    "    misc_english_words_test = misc_english_extractor(df_test)\n",
    "    total_misc = misc_english_words.union(misc_english_words_test)\n",
    "    total_misc_list = list(total_misc)\n",
    "    translated_misc = translate_english_to_nepali(total_misc_list)\n",
    "    misc_dict = dict(zip(total_misc_list, translated_misc))    \n",
    "    misc_dict = {r'\\b' + key + r'\\b': value for key, value in misc_dict.items()}  \n",
    "    df_train['text'] = df_train['text'].replace(misc_dict,regex=True)\n",
    "    df_test['text'] = df_test['text'].replace(misc_dict,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(text):    \n",
    "    text = re.sub(r'[+!@#$%^&*(),.?\":{}|<>~`;/\\\\[\\]\\'|-]+', ' ', text) # Remove all special characters    \n",
    "    text = re.sub(r'[a-zA-Z]', ' ', text)  # Remove the 'a-zA-Z' character\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra whitespace    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',encoding='utf-8')\n",
    "df_test = pd.read_csv('test.csv',encoding='utf-8')\n",
    "df_test.dropna(inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train = df_train[~df_train['label'].str.match(r'^(-|20|11|o|--)$')]\n",
    "df_test = df_test[~df_test['label'].str.match(r'^(-|20|11|o|--)$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emojis to text\n",
    "df_train['text'] = df_train['text'].apply(replace_emojis_with_space)\n",
    "df_test['text'] = df_test['text'].apply(replace_emojis_with_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_handler(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_english_handler(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['text'] != '']\n",
    "df_test = df_test[df_test['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(remove_stop_words)\n",
    "df_test['text'] = df_test['text'].apply(remove_stop_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(final_clean)\n",
    "df_test['text'] = df_test['text'].apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['text'] != '']\n",
    "df_test = df_test[df_test['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_clean.csv',index=False)\n",
    "df_test.to_csv('test_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
