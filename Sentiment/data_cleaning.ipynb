{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',encoding='utf-8')\n",
    "df_test = pd.read_csv('test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True)\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    888\n",
       "0    609\n",
       "2    496\n",
       "o      1\n",
       "-      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1     2378\n",
       "0     2376\n",
       "2     1236\n",
       "-        5\n",
       "20       1\n",
       "11       1\n",
       "o        1\n",
       "--       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['label'].str.match(r'^(-|20|11|o|--)$')]\n",
    "df_test = df_test[~df_test['label'].str.match(r'^(-|20|11|o|--)$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '2'], dtype=object)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '1', '0'], dtype=object)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "# Function to replace emojis with descriptions and add spaces\n",
    "def replace_emojis_with_space(text):\n",
    "    # Replace emojis with descriptions, adding spaces before and after each description\n",
    "    text = demoji.replace_with_desc(text, sep=\"+\")  # Add space between descriptions\n",
    "    text = text.replace('-',' ')\n",
    "    text = text.replace(':',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emojis to text\n",
    "df_train['text'] = df_train['text'].apply(replace_emojis_with_space)\n",
    "df_test['text'] = df_test['text'].apply(replace_emojis_with_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['length'] = df_train['text'].apply(lambda x: len(x))\n",
    "df_test['length'] = df_test['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['length']<=256]\n",
    "df_test = df_test[df_test['length']<=256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Distribution of Text Length')\n",
    "\n",
    "df_train['length'].hist()\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_test['length'].hist()\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['length'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5747.000000\n",
       "mean       83.037063\n",
       "std        55.477971\n",
       "min         3.000000\n",
       "25%        40.000000\n",
       "50%        70.000000\n",
       "75%       113.000000\n",
       "max       256.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_12204\\2063972909.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.drop(columns=['length'], inplace=True,axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_train.drop(columns=['length'], inplace=True,axis=1)\n",
    "df_test.drop(columns=['length'], inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_english_to_nepali(batch):\n",
    "    return GoogleTranslator(source='en', target='ne').translate_batch(batch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_between_plus(text):\n",
    "    # Use regex to find all words between + and +\n",
    "    return re.findall(r'\\+(.*?)\\+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_english_letters(text):\n",
    "    # Check if the text contains at least one English letter (a-z or A-Z)\n",
    "    return bool(re.search(r'[a-zA-Z]', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_english_words(text):\n",
    "    # Check if the text contains at least one English letter (a-z or A-Z)\n",
    "    return re.findall(r'[a-zA-Z\\+]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_extractor(df):\n",
    "    present_english_words = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        sentence = ' '.join(find_english_words(text))\n",
    "        unique_words = set(find_words_between_plus(sentence))\n",
    "        if unique_words:            \n",
    "            present_english_words.extend(unique_words)\n",
    "    return set(present_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_handler():\n",
    "    emoji_words = emoji_extractor(df_train)\n",
    "    emoji_words_test = emoji_extractor(df_test)\n",
    "    total_words = emoji_words.union(emoji_words_test)\n",
    "    total_words_list = list(total_words)\n",
    "    translated_emojis = translate_english_to_nepali(total_words_list)\n",
    "    emoji_dict = dict(zip(total_words_list, translated_emojis))\n",
    "    df_train['text'] = df_train['text'].replace({'\\+': ' '}, regex=True)\n",
    "    df_train['text'] = df_train['text'].replace(emoji_dict,regex=True)\n",
    "    df_test['text'] = df_test['text'].replace({'\\+': ' '}, regex=True)\n",
    "    df_test['text'] = df_test['text'].replace(emoji_dict,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract English words and separate sentences based on Nepali characters\n",
    "def extract_english_words_inbetween(text):\n",
    "    # Regex pattern for detecting Nepali characters and English words\n",
    "    nepali_pattern = r'[\\u0900-\\u097F]+'\n",
    "    english_word_pattern = r'[a-zA-Z]+'\n",
    "    \n",
    "    # Split text by Nepali characters to separate sentences\n",
    "    sentences = re.split(nepali_pattern, text)\n",
    "    \n",
    "    # Now extract English words from each sentence\n",
    "    english_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = re.findall(english_word_pattern, sentence)\n",
    "        if words:  # If there are any English words, store them as a sentence\n",
    "            english_sentences.append(\" \".join(words))\n",
    "    \n",
    "    return english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misc_english_extractor(df):\n",
    "    present_english_words = []\n",
    "    for text in df['text']:\n",
    "        eng = extract_english_words_inbetween(text)\n",
    "        if eng:\n",
    "            present_english_words.extend(eng)\n",
    "    return set(present_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misc_english_handler():\n",
    "    misc_english_words = misc_english_extractor(df_train)\n",
    "    misc_english_words_test = misc_english_extractor(df_test)\n",
    "    total_misc = misc_english_words.union(misc_english_words_test)\n",
    "    total_misc_list = list(total_misc)\n",
    "    translated_misc = translate_english_to_nepali(total_misc_list)\n",
    "    misc_dict = dict(zip(total_misc_list, translated_misc))  \n",
    "    misc_dict = {r'\\b' + key + r'\\b': value for key, value in misc_dict.items()}  \n",
    "    df_train['text'] = df_train['text'].replace(misc_dict,regex=True)\n",
    "    df_test['text'] = df_test['text'].replace(misc_dict,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = {'and':'र','Prachanda':'prach','c':'ग','secondary':'sec'}\n",
    "temp_dict = {r'\\b' + key + r'\\b': value for key, value in temp_dict.items()}\n",
    "\n",
    "temp_df = pd.DataFrame({'text':['boom Prachanda and','secondary']})\n",
    "temp_df['text'] = temp_df['text'].replace(temp_dict,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    boom prach र\n",
       "1             sec\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_english_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This function takes a list of sentences and converts them to nepali using batch translation\n",
    "\n",
    "Problem:\n",
    "The function to translate english to nepali is very slow so translating every sentence even those already in nepali is a waste of time\n",
    "\"\"\"\n",
    "def batch_translate_Older(sentences,batch_size=100):\n",
    "    translated_list = []    \n",
    "    for i in range(0,len(sentences),batch_size):\n",
    "        print(f'Sentence: ',i)\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        translated_sentences = translate_english_to_nepali(batch_sentences)\n",
    "        translated_list.extend(translated_sentences)\n",
    "    return translated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Description: \n",
    "This function takes a list of sentences, looks for sentences containing english words,\n",
    "tags those sentences, converts only those sentences containing english words and merges all\n",
    "the sentences back to a single list with the order preserved\n",
    "\n",
    "Problem:\n",
    "The function that translates english to nepali converts the sentence as a whole which sometimes changes the meaning of the sentence\n",
    "\"\"\"\n",
    "def batch_translate_Old(sentences,batch_size=100):\n",
    "    translated_list = []\n",
    "    index = 0\n",
    "    while index< len(sentences):\n",
    "        batch_sentences = []\n",
    "        non_batch_sentences = []\n",
    "        need_translation = []        \n",
    "        batch_idx = 0\n",
    "        while batch_idx<batch_size and index<len(sentences):\n",
    "            if contains_english_letters(sentences[index]):\n",
    "                batch_sentences.append(sentences[index])\n",
    "                need_translation.append(1)\n",
    "            else:\n",
    "                non_batch_sentences.append(sentences[index])\n",
    "                need_translation.append(0)\n",
    "            batch_idx+=1\n",
    "            index+=1\n",
    "        translated_sentences = translate_english_to_nepali(batch_sentences)\n",
    "        batching_idx = 0\n",
    "        non_batching_idx = 0        \n",
    "        for this_idx in range(batch_idx):\n",
    "            if need_translation[this_idx]:                \n",
    "                translated_list.append(translated_sentences[batching_idx])\n",
    "                batching_idx +=1\n",
    "            else:                \n",
    "                translated_list.append(non_batch_sentences[non_batching_idx])\n",
    "                non_batching_idx+=1\n",
    "    return translated_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#stopwords_list = set(nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('nepali'))\n",
    "\n",
    "stopwords_list = set(nltk.corpus.stopwords.words('nepali'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "# Initialize an empty list for extra stopwords\n",
    "extra_stop_words = []\n",
    "\n",
    "# Open the file with UTF-8 encoding\n",
    "with codecs.open('nepali_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip whitespace and add the word to the list\n",
    "        extra_stop_words.append(line.strip())\n",
    "stopwords_list.update(extra_stop_words)\n",
    "stopwords_list = set(stopwords_list)\n",
    "stopwords_list.remove('राम्रो')\n",
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = text.lower()\n",
    "    filtered_words = [word for word in text.split() if word not in stopwords_list]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(remove_stop_words)\n",
    "df_test['text'] = df_test['text'].apply(remove_stop_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "def clean_text(text):\n",
    "    #text = re.sub(r'[^\\w\\s\\u0900-\\u097F]+', '', text)  # Everything except letter, numbers and nepali\n",
    "    text = re.sub(r'[+!@#$%^&*(),.?\":{}|<>~`;/\\\\[\\]\\'|-]+', ' ', text) # Remove all special characters\n",
    "    #text = re.sub(r'\\b[a-zA-Z]+\\b', '', text)    # Remove English alphabets\n",
    "    text = re.sub(r'।', ' ', text)  # Remove the '।' character\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra whitespace\n",
    "    #text = emoji_pattern.sub(r'', text)  # Remove emojis\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_12204\\2630466235.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['text'] = df_train['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['text'] != '']\n",
    "df_test = df_test[df_test['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_clean.csv',index=False)\n",
    "df_test.to_csv('test_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,text in enumerate(X):\n",
    "  if not isinstance(text, str):\n",
    "    print(text)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
