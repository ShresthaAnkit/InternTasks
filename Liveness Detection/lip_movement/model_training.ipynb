{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import librosa\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from scipy.linalg import svd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "from scipy.spatial import distance as dist\n",
    "from scipy.signal import find_peaks\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye_landmarks):\n",
    "    # Compute the Euclidean distances between the vertical eye landmarks\n",
    "    A = dist.euclidean(eye_landmarks[1], eye_landmarks[5])\n",
    "    B = dist.euclidean(eye_landmarks[2], eye_landmarks[4])\n",
    "    # Compute the Euclidean distance between the horizontal eye landmarks\n",
    "    C = dist.euclidean(eye_landmarks[0], eye_landmarks[3])\n",
    "    # Compute the eye aspect ratio\n",
    "    ear = (A + B) / (C)\n",
    "    return ear\n",
    "def extract_video_features(video_path):\n",
    "    \"\"\"Extracts lip landmarks from the video using MediaPipe.\"\"\"\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Get video frame rate using OpenCV    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second    \n",
    "    upper_lip_points = [185,40,39,37,0,267,269,270,409]\n",
    "    lower_lip_points = [146,91,181,84,17,314,405,321,375]    \n",
    "\n",
    "    # Variables for blink detection\n",
    "    EYE_STATUS = 0  # OPEN = 0 and CLOSED = 1\n",
    "    total_blinks = 0  # Total number of blinks detected\n",
    "    EAR = 0\n",
    "    # Landmark indices for left and right eyes\n",
    "    LEFT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "    RIGHT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "    lip_features = []\n",
    "    # Constants for blink detection\n",
    "    EAR_THRESHOLD = 0.25  # Eye aspect ratio threshold for blink detection    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            for landmarks in results.multi_face_landmarks:                           \n",
    "                face_width = math.sqrt((landmarks.landmark[323].x - landmarks.landmark[93].x)**2 + (landmarks.landmark[323].y - landmarks.landmark[93].y)**2)\n",
    "                \n",
    "                # Lip movement feature extraction\n",
    "                upper_lips = [(lm.x, lm.y) for i, lm in enumerate(landmarks.landmark) if i in upper_lip_points]\n",
    "                lower_lips = [(lm.x, lm.y) for i, lm in enumerate(landmarks.landmark) if i in lower_lip_points]\n",
    "                distances = [math.sqrt((ul[0] - ll[0])**2 + (ul[1] - ll[1])**2) / face_width for ul, ll in zip(upper_lips, lower_lips)]                            \n",
    "                lip_features.append(np.array(distances))  # Append the list of distances for each frame to lip_features\n",
    "\n",
    "                # Eye movement feature extraction                \n",
    "                # Extract landmarks for left and right eyes\n",
    "                left_eye = []\n",
    "                right_eye = []\n",
    "                for idx in LEFT_EYE_INDICES:\n",
    "                    landmark = landmarks.landmark[idx]\n",
    "                    x = int(landmark.x * frame.shape[1])\n",
    "                    y = int(landmark.y * frame.shape[0])\n",
    "                    left_eye.append((x, y))\n",
    "                for idx in RIGHT_EYE_INDICES:\n",
    "                    landmark = landmarks.landmark[idx]\n",
    "                    x = int(landmark.x * frame.shape[1])\n",
    "                    y = int(landmark.y * frame.shape[0])\n",
    "                    right_eye.append((x, y))\n",
    "\n",
    "                # Calculate EAR for both eyes\n",
    "                left_ear = eye_aspect_ratio(left_eye)\n",
    "                right_ear = eye_aspect_ratio(right_eye)\n",
    "                EAR = (left_ear + right_ear) / 2.0\n",
    "                \n",
    "                if EAR < EAR_THRESHOLD:\n",
    "                    EYE_STATUS = 1                    \n",
    "                else:\n",
    "                    if EYE_STATUS == 1:\n",
    "                        total_blinks += 1\n",
    "                    EYE_STATUS = 0                    \n",
    "    cap.release()\n",
    "    return np.array(lip_features),total_blinks, fps\n",
    "def get_audio_energy(video_path,fps):\n",
    "    # Load audio using librosa\n",
    "    y, sr = librosa.load(video_path, sr=16000, mono=True)        \n",
    "    \n",
    "    # Calculate hop_length and frame_length to match video frames\n",
    "    hop_length = int(sr / fps)  # Hop length to match video frame rate\n",
    "    frame_length = int(sr * 0.05)  # 50ms window (can be adjusted)\n",
    "    \n",
    "    # Calculate short-time energy\n",
    "    energy = np.array([\n",
    "        sum(abs(y[i:i+frame_length])**2)\n",
    "        for i in range(0, len(y), hop_length)\n",
    "    ])\n",
    "    \n",
    "    return energy\n",
    "# Min-max normalization function\n",
    "def min_max_normalize(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "def moving_average(data, window_size):\n",
    "    # Ensure the window size is odd\n",
    "    if window_size % 2 == 0:\n",
    "        raise ValueError(\"Window size must be odd to maintain symmetry.\")\n",
    "    \n",
    "    # Create a window of ones and normalize it\n",
    "    window = np.ones(window_size) / window_size\n",
    "    \n",
    "    # Pad the data to handle edges\n",
    "    pad_size = window_size // 2\n",
    "    padded_data = np.pad(data, (pad_size, pad_size), mode='edge')  # Reflect padding\n",
    "    \n",
    "    # Apply convolution to compute the moving average\n",
    "    smoothed_data = np.convolve(padded_data, window, mode='valid')\n",
    "    \n",
    "    return smoothed_data\n",
    "def detect_peaks(data):\n",
    "    # Compute absolute difference between consecutive frames\n",
    "    movement = np.abs(np.diff(data))\n",
    "    peaks, _ = find_peaks(movement, height=np.mean(movement) + np.std(movement),\n",
    "                      prominence=0.05, width=1, distance=2)\n",
    "    return peaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_features(video_path):\n",
    "    start_delay = 10\n",
    "    window_size = 9\n",
    "    #print(f'[Processing] Video')\n",
    "\n",
    "    lip_feat,blink_count,fps = extract_video_features(video_path)\n",
    "    mean_dist = np.sum(lip_feat,axis=1)\n",
    "    \n",
    "\n",
    "    audio_energy = get_audio_energy(video_path,fps)\n",
    "\n",
    "    min_len = min(len(mean_dist),len(audio_energy))\n",
    "    normalized_dist = min_max_normalize(mean_dist)[start_delay:min_len]\n",
    "    normalized_audio = min_max_normalize(audio_energy)[start_delay:min_len]    \n",
    "    avg_dist = moving_average(normalized_dist, window_size)\n",
    "    avg_audio = moving_average(normalized_audio, window_size)\n",
    "\n",
    "    # Detect lip movement\n",
    "    lip_movements = len(detect_peaks(avg_dist))\n",
    "    \n",
    "    # Detect Audio Change\n",
    "    audio_changes = len(detect_peaks(audio_energy))\n",
    "\n",
    "    # Calculate Pearson correlation coefficient\n",
    "    correlation, _ = spearmanr(avg_dist, avg_audio)    \n",
    "\n",
    "    return blink_count,lip_movements,audio_changes,correlation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['blink_count','lip_movements','audio_changes','correlation','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61dd5f24a14d0d2b3b211c83.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61de4edf6292055ac305e885.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61e28d132e6cac25f79a3346.MOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61ece94de8e9ba2b3bc01c59.mkv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61f276f01c8b5c57e89244a1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61f5617b66629e059dbeaef5.webm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--61f82a2956ef241fa1f94ff0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--6203fa2ad1a403449593c5cb.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001d815c0--620984e9f002b8749797ec67.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing] C:/Users/aanki/Downloads/live_videos/real/0001db3fa7--61e8f5380569a62870798ef9.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    }
   ],
   "source": [
    "test = 'real'\n",
    "sets = ['real','fake']\n",
    "\n",
    "#video_folder_path = f'C:/Users/aanki/Downloads/live_videos'\n",
    "\n",
    "start_delay = 10\n",
    "for test_set in sets:\n",
    "    video_folder_path = f'D:/Programming/Python/AI/Basics/AMNIL Tech/Liveness Detection/lip_movement/video/{test_set}'\n",
    "    #video_folder_path = f'C:/Users/aanki/Downloads/live_videos/{test_set}'\n",
    "    videos = os.listdir(f'{video_folder_path}')\n",
    "    for video in videos:\n",
    "        video_path = f'{video_folder_path}/{video}'\n",
    "        print(f'[Processing] {video_path}')\n",
    "        blink_count,lip_movements,audio_changes,correlation = get_video_features(video_path)      \n",
    "        label = 1 if test_set == 'real' else 0           \n",
    "        df.loc[len(df)] = [blink_count,lip_movements,audio_changes,correlation,label]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['blink_count','lip_movements','audio_changes','correlation']]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Python\\AI\\Basics\\intern-phase-2\\intern-phase-2\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Train model (Random Forest)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [1. 1. 0. 1. 1.]\n",
      "Actual:  [1. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Pred: ',y_pred)\n",
    "print('Actual: ',y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.00\n",
      "Pred:  [1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Actual:  [1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print('Pred: ',y_pred_train)\n",
    "print('Actual: ',y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Python\\AI\\Basics\\intern-phase-2\\intern-phase-2\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred_X = model.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred_X)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanki\\AppData\\Local\\Temp\\ipykernel_38004\\2411243942.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(video_path, sr=16000, mono=True)\n"
     ]
    }
   ],
   "source": [
    "d_path = r\"C:\\Users\\aanki\\OneDrive\\Pictures\\Camera Roll\\WIN_20250129_17_58_48_Pro.mp4\"\n",
    "\n",
    "blink_count,lip_movements,audio_changes,correlation = get_video_features(d_path)\n",
    "df_test = pd.DataFrame(columns=['blink_count','lip_movements','audio_changes','correlation'])\n",
    "df_test.loc[len(df_test)] = [blink_count,lip_movements,audio_changes,correlation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Python\\AI\\Basics\\intern-phase-2\\intern-phase-2\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blink_count</th>\n",
       "      <th>lip_movements</th>\n",
       "      <th>audio_changes</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.60786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blink_count  lip_movements  audio_changes  correlation\n",
       "0          0.0            1.0            8.0      0.60786"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern-phase-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
