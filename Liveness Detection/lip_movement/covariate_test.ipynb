{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import librosa\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import svd\n",
    "from moviepy import VideoFileClip\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Step 1: Load Video and Extract Audio\n",
    "def extract_audio_from_video(video_path, audio_path=r\"D:\\Programming\\Python\\AI\\Basics\\intern-phase-2\\Liveness Detection\\lip_movement\\audio\"):\n",
    "    # Extract video name (without extension)\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    # Define the audio file name with the same base name as the video\n",
    "    audio_path = os.path.join(audio_path, f\"{video_name}.wav\")\n",
    "    # Check if the audio file already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"Audio file already exists: {audio_path}\")\n",
    "        return audio_path\n",
    "    command = [\n",
    "        'ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '44100', '-ac', '2', audio_path\n",
    "    ]\n",
    "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return audio_path\n",
    "\n",
    "# Step 2: Extract Audio Features\n",
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"Extracts MFCC features from the audio.\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Shape: (13, n_frames)\n",
    "    return mfccs.T  # Transpose to have frames along rows\n",
    "\n",
    "# Step 3: Extract Lip Movement Features\n",
    "def extract_lip_features(video_path):\n",
    "    \"\"\"Extracts lip landmarks from the video using MediaPipe.\"\"\"\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    lip_features = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            for landmarks in results.multi_face_landmarks:\n",
    "                # Extract lip landmarks (e.g., indices 0-16)\n",
    "                lips = [(lm.x, lm.y) for i, lm in enumerate(landmarks.landmark) if i in range(0, 17)]\n",
    "                lip_features.append(np.array(lips).flatten())\n",
    "    cap.release()\n",
    "    return np.array(lip_features)\n",
    "\n",
    "# Step 4: Align and Normalize Features\n",
    "def align_and_normalize_features(audio_features, video_features):\n",
    "    \"\"\"Aligns audio and video features by resampling and normalizing.\"\"\"\n",
    "    n_video_frames = video_features.shape[0]\n",
    "    aligned_audio_features = resample(audio_features, n_video_frames)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    aligned_audio_features = scaler.fit_transform(aligned_audio_features)\n",
    "    video_features = scaler.fit_transform(video_features)\n",
    "    return aligned_audio_features, video_features\n",
    "\n",
    "# Step 5: Perform Co-Inertia Analysis\n",
    "def perform_coia(audio_features, video_features):\n",
    "    \"\"\"Performs Co-Inertia Analysis and returns the co-inertia score.\"\"\"\n",
    "    cross_covariance = np.dot(audio_features.T, video_features)\n",
    "    _, singular_values, _ = svd(cross_covariance)\n",
    "    co_inertia_score = np.sum(singular_values)\n",
    "    return co_inertia_score\n",
    "def calculate_L(p_hat, D):\n",
    "    \"\"\"\n",
    "    Calculates the value of L based on the given equation.\n",
    "\n",
    "    Args:\n",
    "        p_hat: A numpy array representing the estimated probability values.\n",
    "        D: The value of D for the calculation.\n",
    "\n",
    "    Returns:\n",
    "        The calculated value of L.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate p_ref\n",
    "    p_ref = np.max(p_hat)\n",
    "\n",
    "    # Calculate f(p_hat, i)\n",
    "    f_p_hat = np.where(p_hat <= p_ref, 1, 0)\n",
    "\n",
    "    # Calculate the sum of f(p_hat, i)\n",
    "    sum_f_p_hat = np.sum(f_p_hat)\n",
    "\n",
    "    # Calculate mean(p_hat)\n",
    "    mean_p_hat = np.mean(p_hat)\n",
    "\n",
    "    # Calculate L\n",
    "    L = (sum_f_p_hat / (2 * D + 1)) * ((p_ref / mean_p_hat) - 1)\n",
    "\n",
    "    return L\n",
    "\n",
    "# Step 6: Main Function for Liveness Detection\n",
    "def detect_liveness(video_path, threshold=0.5):\n",
    "    \"\"\"Main function to detect liveness using COIA.\"\"\"\n",
    "    # Extract audio from video\n",
    "    print(\"[INFO] Extracting audio...\")\n",
    "    audio_path = extract_audio_from_video(video_path)\n",
    "    \n",
    "    # Extract audio features\n",
    "    print(\"[INFO] Extracting audio features...\")\n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    \n",
    "    # Extract lip movement features\n",
    "    print(\"[INFO] Extracting lip movement features...\")\n",
    "    lip_features = extract_lip_features(video_path)\n",
    "    \n",
    "    # Align and normalize features\n",
    "    print(\"[INFO] Aligning and normalizing features...\")\n",
    "    aligned_audio, aligned_video = align_and_normalize_features(audio_features, lip_features)\n",
    "    \n",
    "    # Perform Co-Inertia Analysis\n",
    "    print(\"[INFO] Performing Co-Inertia Analysis...\")\n",
    "    co_inertia_score = perform_coia(aligned_audio, aligned_video)\n",
    "    print(f\"[INFO] Co-Inertia Score: {co_inertia_score}\")\n",
    "    L = calculate_L(co_inertia_score,10)\n",
    "    print(f\"[INFO] Threshold: {L}\")\n",
    "    \n",
    "    # Classify based on the threshold\n",
    "    if L > threshold:\n",
    "        print(\"[RESULT] Live (Synchronized)\")\n",
    "    else:\n",
    "        print(\"[RESULT] Fake (Desynchronized)\")\n",
    "\n",
    "# Example Usage\n",
    "video_path = r\"D:\\Programming\\Python\\AI\\Basics\\intern-phase-2\\Liveness Detection\\lip_movement\\video\\fake3.mp4\"\n",
    "detect_liveness(video_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern-phase-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
